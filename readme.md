# ğŸ§  Barlow Twins + Transformer Decoder for Image Captioning

## This project is part of an ongoing experimental research effort exploring how self-supervised visual features (via Barlow Twins) can be used effectively with Transformer-based decoders for image     captioning tasks. The encoder is built on EfficientNetV2-S, with a projection head during pretraining. The decoder is a custom Transformer stack.

âš ï¸ Note: This repository is not production-ready. It is under active development and research.
I'm actively seeking guidance, feedback, and open to collaborations from the community.

This repository presents a modular, experimental image captioning pipeline that combines **Barlow Twins self-supervised learning**, **EfficientNetV2-S** as a visual backbone, and a custom **Transformer Decoder** to generate human-like image captions.  
It explores the separation of representation learning and caption generation â€” a powerful approach for generalizing across domains.

---

## ğŸš€ Highlights

- ğŸ§¬ **Barlow Twins Pretraining**: Self-supervised learning with projection head (512 â†’ 128), trained on patch-wise visual tokens
- âš¡ **EfficientNetV2-S**: Encoder backbone (with projection head **only during pretraining**)
- ğŸ§  **Transformer Decoder**: 6-layer, 8-head, GELU-activated decoder with learnable positional encodings
- ğŸ“ **Custom Tokenizer & Loader**: Token-level caption generation using `word2idx`, `<SOS>`/`<EOS>` logic, with fallback image handling
- ğŸ“¦ **Dataset**: Microsoft COCO 2017 for both training and evaluation
- âš™ï¸ **Fully PyTorch** â€” no external vision-language libraries used
