{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b71ade",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      " Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL 0 - SELECT DEVICE & LOAD DEPENDENCIES\n",
    "\n",
    "!pip install -q timm\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import timm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474934f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Patch token shape: torch.Size([1, 64, 256])\n",
      " Encoder weights loaded with strict=True\n",
      " Encoder ready (feature dim = 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3515/3045962482.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=True)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Encoder Only (decoder-ready, no projection head)\n",
    "\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  Encoder that returns patch-wise feature map: (B, 64, C)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"efficientnetv2_s\",\n",
    "            pretrained=False,\n",
    "            features_only=True\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # ensures 8×8 spatial output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)[-1]              # (B, C, H, W) → final block\n",
    "        x = self.pool(x)                      # (B, C, 8, 8)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1)             # (B, 8, 8, C)\n",
    "        x = x.reshape(B, H * W, C)            # (B, 64, C)\n",
    "        return x\n",
    "\n",
    "# Instantiate encoder first\n",
    "encoder = Encoder().to(device)\n",
    "\n",
    "#  Dynamically detect feature dim (C) from dummy input\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, 256, 256).to(device)\n",
    "    out = encoder(dummy)\n",
    "    feature_dim = out.shape[-1]  # usually 1280 for effnetv2_s\n",
    "    print(f\" Patch token shape: {out.shape}\")  # (1, 64, 1280)\n",
    "\n",
    "#  Load encoder weights with fallback\n",
    "try:\n",
    "    encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=True)\n",
    "    print(\" Encoder weights loaded with strict=True\")\n",
    "except RuntimeError as e:\n",
    "    print(\" Strict loading failed:\", e)\n",
    "    print(\" Retrying with strict=False\")\n",
    "    encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=False)\n",
    "\n",
    "print(f\" Encoder ready (feature dim = {feature_dim})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bea99b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 2 - CAPTION PROCESSING\n",
    "\n",
    "\n",
    "with open(\"captions_train2017.json\", 'r') as f:\n",
    "    annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "captions_dict = {}\n",
    "for ann in annotations:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    cap = ann[\"caption\"]\n",
    "    captions_dict.setdefault(img_id, []).append(cap)\n",
    "\n",
    "# Basic tokenizer & vocab\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "word_freq = {}\n",
    "for caps in captions_dict.values():\n",
    "    for cap in caps:\n",
    "        for token in tokenize(cap):\n",
    "            word_freq[token] = word_freq.get(token, 0) + 1\n",
    "\n",
    "# Build vocab\n",
    "vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "for word, freq in word_freq.items():\n",
    "    if freq >= 5:  # min frequency cutoff\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "word2idx = vocab\n",
    "idx2word = {idx: word for word, idx in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"word2idx\": word2idx, \"idx2word\": idx2word}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9249f382",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 3 - DATASET + DATALOADER\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  #  allows loading broken JPEGs\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, folder, image2caption, word2idx, transform):\n",
    "        self.folder = folder\n",
    "        self.mapping = list(image2caption.items())\n",
    "        self.word2idx = word2idx\n",
    "        self.transform = transform\n",
    "        self.fallback_count = 0  #  optional: track black image fallbacks\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_id, captions = self.mapping[i]\n",
    "        img_path = os.path.join(self.folder, f\"{img_id:012}.jpg\")\n",
    "\n",
    "        #  Safe image load with grayscale/corrupt fallback\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            self.fallback_count += 1\n",
    "            print(f\" Fallback: Could not load {img_path} — {e}\")\n",
    "            image = Image.new(\"RGB\", (256, 256), color=(0, 0, 0))  # black dummy\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        #  Caption tokenization\n",
    "        caption = [\"<SOS>\"] + random.choice(captions).lower().strip().split() + [\"<EOS>\"]\n",
    "        tokens = [self.word2idx.get(w.strip(\".,!?\"), self.word2idx[\"<UNK>\"]) for w in caption]\n",
    "        tokens = tokens[:20] + [self.word2idx[\"<PAD>\"]] * (20 - len(tokens))\n",
    "\n",
    "        return image, torch.tensor(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "#  Standard transform (resize, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "#  Instantiate dataset\n",
    "dataset = ImageCaptionDataset(\"images\", captions_dict, word2idx, transform)\n",
    "\n",
    "#  FINAL DataLoader with high batch size + PIL-safe settings\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,      #  set to 1024 if you tested and confirmed it's safe\n",
    "    shuffle=True,\n",
    "    num_workers=0,       #  Single-threaded (PIL-safe)\n",
    "    pin_memory=False     #  Avoid async GPU transfer issues\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32dfb3bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CELL 4 - TRANSFORMER DECODER\n",
    "\n",
    "class CaptionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim, hidden_dim=512, num_layers=6, nhead=8, max_len=20):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(max_len, hidden_dim))\n",
    "        self.img_proj = nn.Linear(feature_dim, hidden_dim)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead, activation='gelu', batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, image_tokens, caption_tokens):\n",
    "        B, T = caption_tokens.shape\n",
    "        tgt = self.token_embed(caption_tokens) + self.pos_embed[:T]\n",
    "        memory = self.img_proj(image_tokens)\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(caption_tokens.device)\n",
    "        return self.fc_out(self.decoder(tgt, memory, tgt_mask=tgt_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83527fbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 5 - ENCODER + DECODER \n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        image_tokens = self.encoder(images)           # (B, 64, C)\n",
    "        return self.decoder(image_tokens, captions)   # (B, T, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0942f1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3515/4127236421.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 - LOSS, OPTIMIZER & AMP \n",
    "\n",
    "PAD_ID = word2idx[\"<PAD>\"]\n",
    "\n",
    "decoder = CaptionDecoder(\n",
    "    vocab_size=len(word2idx),\n",
    "    feature_dim=encoder(torch.randn(1, 3, 256, 256).to(device)).shape[-1]\n",
    ").to(device)\n",
    "\n",
    "model = ImageCaptioningModel(encoder, decoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.decoder.parameters(), \"lr\": 1e-4}\n",
    "])\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb59acd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3515/2849251408.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_3515/2849251408.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 000 | Loss: 9.7175 | GPU Mem: 2.23 GB\n",
      " Step 005 | Loss: 8.1538 | GPU Mem: 2.23 GB\n",
      " Step 010 | Loss: 7.3898 | GPU Mem: 2.23 GB\n",
      " Step 015 | Loss: 7.0409 | GPU Mem: 2.23 GB\n",
      " Step 020 | Loss: 6.7757 | GPU Mem: 2.23 GB\n",
      " Step 025 | Loss: 6.5165 | GPU Mem: 2.23 GB\n",
      " Step 030 | Loss: 6.2754 | GPU Mem: 2.23 GB\n",
      " Step 035 | Loss: 6.0958 | GPU Mem: 2.23 GB\n",
      " Step 040 | Loss: 5.9553 | GPU Mem: 2.23 GB\n",
      " Step 045 | Loss: 5.8152 | GPU Mem: 2.23 GB\n",
      " Step 050 | Loss: 5.6288 | GPU Mem: 2.23 GB\n",
      " Step 055 | Loss: 5.4940 | GPU Mem: 2.23 GB\n",
      " Step 060 | Loss: 5.4055 | GPU Mem: 2.23 GB\n",
      " Step 065 | Loss: 5.3146 | GPU Mem: 2.23 GB\n",
      " Step 070 | Loss: 5.2387 | GPU Mem: 2.23 GB\n",
      " Step 075 | Loss: 5.1555 | GPU Mem: 2.23 GB\n",
      " Step 080 | Loss: 5.0369 | GPU Mem: 2.23 GB\n",
      " Step 085 | Loss: 5.0171 | GPU Mem: 2.23 GB\n",
      " Step 090 | Loss: 4.8977 | GPU Mem: 2.23 GB\n",
      " Step 095 | Loss: 4.8328 | GPU Mem: 2.23 GB\n",
      " Step 100 | Loss: 4.7934 | GPU Mem: 2.23 GB\n",
      " Step 105 | Loss: 4.7507 | GPU Mem: 2.23 GB\n",
      " Step 110 | Loss: 4.7290 | GPU Mem: 2.23 GB\n",
      " Step 115 | Loss: 4.6598 | GPU Mem: 1.62 GB\n",
      "\n",
      " Epoch 1/10 | Avg Loss: 5.809684\n",
      "\n",
      " Checkpoint saved to checkpoints/captioning_latest.pth\n",
      " Step 000 | Loss: 4.6206 | GPU Mem: 2.22 GB\n",
      " Step 005 | Loss: 4.5940 | GPU Mem: 2.23 GB\n",
      " Step 010 | Loss: 4.5894 | GPU Mem: 2.23 GB\n",
      " Step 015 | Loss: 4.5344 | GPU Mem: 2.23 GB\n",
      " Step 020 | Loss: 4.4488 | GPU Mem: 2.23 GB\n",
      " Step 025 | Loss: 4.3859 | GPU Mem: 2.23 GB\n",
      " Step 030 | Loss: 4.3951 | GPU Mem: 2.23 GB\n",
      " Step 035 | Loss: 4.3337 | GPU Mem: 2.23 GB\n",
      " Step 040 | Loss: 4.2768 | GPU Mem: 2.23 GB\n",
      " Step 045 | Loss: 4.2232 | GPU Mem: 2.23 GB\n",
      " Step 050 | Loss: 4.2042 | GPU Mem: 2.23 GB\n",
      " Step 055 | Loss: 4.1765 | GPU Mem: 2.23 GB\n",
      " Step 060 | Loss: 4.1255 | GPU Mem: 2.23 GB\n",
      " Step 065 | Loss: 4.0819 | GPU Mem: 2.23 GB\n",
      " Step 070 | Loss: 4.0907 | GPU Mem: 2.23 GB\n",
      " Step 075 | Loss: 4.0267 | GPU Mem: 2.23 GB\n",
      " Step 080 | Loss: 3.9702 | GPU Mem: 2.23 GB\n",
      " Step 085 | Loss: 3.9764 | GPU Mem: 2.23 GB\n",
      " Step 090 | Loss: 3.9857 | GPU Mem: 2.23 GB\n",
      " Step 095 | Loss: 3.8756 | GPU Mem: 2.23 GB\n",
      " Step 100 | Loss: 3.9496 | GPU Mem: 2.23 GB\n",
      " Step 105 | Loss: 3.8111 | GPU Mem: 2.23 GB\n",
      " Step 110 | Loss: 3.7901 | GPU Mem: 2.23 GB\n",
      " Step 115 | Loss: 3.8075 | GPU Mem: 1.62 GB\n",
      "\n",
      " Epoch 2/10 | Avg Loss: 4.171448\n",
      "\n",
      " Checkpoint saved to checkpoints/captioning_latest.pth\n",
      " Step 000 | Loss: 3.8314 | GPU Mem: 2.22 GB\n"
     ]
    }
   ],
   "source": [
    "# CELL 7 - TRAINING LOOP\n",
    "\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "EPOCHS = 10\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"captioning_latest.pth\")\n",
    "\n",
    "#  Resume logic\n",
    "start_epoch = 0\n",
    "scaler = GradScaler()\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    print(f\" Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "#  Training loop\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, (images, captions) in enumerate(loader):\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(images, captions[:, :-1])  # input\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.size(-1)),\n",
    "                captions[:, 1:].reshape(-1)\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #  Live GPU memory print (once per 5 steps)\n",
    "        if step % 5 == 0:\n",
    "            mem_gb = torch.cuda.memory_allocated() / 1024**3\n",
    "            print(f\" Step {step:03d} | Loss: {loss.item():.4f} | GPU Mem: {mem_gb:.2f} GB\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"\\n Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.6f}\\n\")\n",
    "\n",
    "    #  Save checkpoint after each epoch\n",
    "    ckpt = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'config': {\n",
    "            'vocab_size': len(word2idx),\n",
    "            'hidden_dim': 512,\n",
    "            'num_heads': 8,\n",
    "            'num_layers': 6\n",
    "        }\n",
    "    }\n",
    "    torch.save(ckpt, checkpoint_path)\n",
    "    print(f\" Checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4d14c2-7b78-44cd-8b74-f2d3eec90567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Encoder  # use your actual Encoder class definition\n",
    "\n",
    "# Load full training checkpoint\n",
    "ckpt = torch.load(\"checkpoints/captioning_latest.pth\", map_location=\"cpu\")\n",
    "\n",
    "# Rebuild encoder\n",
    "encoder = Encoder()\n",
    "encoder.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "\n",
    "# Save entire encoder object as .pt (not just weights)\n",
    "torch.save(encoder, \"encoder_epoch_52.pt\")\n",
    "print(\" Saved fine-tuned encoder to encoder_epoch_52.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eacc46-3d22-40b9-8c30-2cb0fc10de26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
