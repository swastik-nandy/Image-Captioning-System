{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "80b71ade",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n\ud83d\udd25 Using device: cuda\n"
        }
      ],
      "source": "# CELL 0 - SELECT DEVICE & LOAD DEPENDENCIES\n\n!pip install -q timm\nimport os\nimport json\nimport pickle\nimport torch\nimport random\nimport timm\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\ud83d\udd25 Using device: {device}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "474934f4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\ud83e\uddea Patch token shape: torch.Size([1, 64, 256])\n\u2705 Encoder weights loaded with strict=True\n\ud83e\udde0 Encoder ready (feature dim = 256)\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/tmp/ipykernel_3515/3045962482.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=True)\n"
        }
      ],
      "source": "# Cell 1 - Encoder Only (decoder-ready, no projection head)\n\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# \u2705 Encoder that returns patch-wise feature map: (B, 64, C)\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"efficientnetv2_s\",\n            pretrained=False,\n            features_only=True\n        )\n        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # ensures 8\u00d78 spatial output\n\n    def forward(self, x):\n        x = self.backbone(x)[-1]              # (B, C, H, W) \u2192 final block\n        x = self.pool(x)                      # (B, C, 8, 8)\n        B, C, H, W = x.shape\n        x = x.permute(0, 2, 3, 1)             # (B, 8, 8, C)\n        x = x.reshape(B, H * W, C)            # (B, 64, C)\n        return x\n\n# Instantiate encoder first\nencoder = Encoder().to(device)\n\n# \ud83d\udd0d Dynamically detect feature dim (C) from dummy input\nwith torch.no_grad():\n    dummy = torch.randn(1, 3, 256, 256).to(device)\n    out = encoder(dummy)\n    feature_dim = out.shape[-1]  # usually 1280 for effnetv2_s\n    print(f\"\ud83e\uddea Patch token shape: {out.shape}\")  # (1, 64, 1280)\n\n# \ud83d\udcbe Load encoder weights with fallback\ntry:\n    encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=True)\n    print(\"\u2705 Encoder weights loaded with strict=True\")\nexcept RuntimeError as e:\n    print(\"\u26a0\ufe0f Strict loading failed:\", e)\n    print(\"\ud83d\udd01 Retrying with strict=False\")\n    encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=False)\n\nprint(f\"\ud83e\udde0 Encoder ready (feature dim = {feature_dim})\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3bea99b0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": "# CELL 2 - CAPTION PROCESSING\n\n\nwith open(\"captions_train2017.json\", 'r') as f:\n    annotations = json.load(f)[\"annotations\"]\n\ncaptions_dict = {}\nfor ann in annotations:\n    img_id = ann[\"image_id\"]\n    cap = ann[\"caption\"]\n    captions_dict.setdefault(img_id, []).append(cap)\n\n# Basic tokenizer & vocab\ndef tokenize(text):\n    return text.lower().strip().split()\n\nword_freq = {}\nfor caps in captions_dict.values():\n    for cap in caps:\n        for token in tokenize(cap):\n            word_freq[token] = word_freq.get(token, 0) + 1\n\n# Build vocab\nvocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\nfor word, freq in word_freq.items():\n    if freq >= 5:  # min frequency cutoff\n        vocab[word] = len(vocab)\n\nword2idx = vocab\nidx2word = {idx: word for word, idx in vocab.items()}\nvocab_size = len(vocab)\n\nwith open(\"vocab.pkl\", \"wb\") as f:\n    pickle.dump({\"word2idx\": word2idx, \"idx2word\": idx2word}, f)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9249f382",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": "# CELL 3 - DATASET + DATALOADER\n\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True  # \u2705 allows loading broken JPEGs\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, folder, image2caption, word2idx, transform):\n        self.folder = folder\n        self.mapping = list(image2caption.items())\n        self.word2idx = word2idx\n        self.transform = transform\n        self.fallback_count = 0  # \ud83e\udde0 optional: track black image fallbacks\n\n    def __getitem__(self, i):\n        img_id, captions = self.mapping[i]\n        img_path = os.path.join(self.folder, f\"{img_id:012}.jpg\")\n\n        # \u2705 Safe image load with grayscale/corrupt fallback\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            self.fallback_count += 1\n            print(f\"\u26a0\ufe0f Fallback: Could not load {img_path} \u2014 {e}\")\n            image = Image.new(\"RGB\", (256, 256), color=(0, 0, 0))  # black dummy\n\n        image = self.transform(image)\n\n        # \ud83e\uddfe Caption tokenization\n        caption = [\"<SOS>\"] + random.choice(captions).lower().strip().split() + [\"<EOS>\"]\n        tokens = [self.word2idx.get(w.strip(\".,!?\"), self.word2idx[\"<UNK>\"]) for w in caption]\n        tokens = tokens[:20] + [self.word2idx[\"<PAD>\"]] * (20 - len(tokens))\n\n        return image, torch.tensor(tokens)\n\n    def __len__(self):\n        return len(self.mapping)\n\n# \u2705 Standard transform (resize, normalize)\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# \u2705 Instantiate dataset\ndataset = ImageCaptionDataset(\"images\", captions_dict, word2idx, transform)\n\n# \u2705 FINAL DataLoader with high batch size + PIL-safe settings\nloader = DataLoader(\n    dataset,\n    batch_size=1024,      # \ud83e\udde0 set to 1024 if you tested and confirmed it's safe\n    shuffle=True,\n    num_workers=0,       # \u2705 Single-threaded (PIL-safe)\n    pin_memory=False     # \u2705 Avoid async GPU transfer issues\n)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "32dfb3bc",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": "\n# CELL 4 - TRANSFORMER DECODER\n\nclass CaptionDecoder(nn.Module):\n    def __init__(self, vocab_size, feature_dim, hidden_dim=512, num_layers=6, nhead=8, max_len=20):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_embed = nn.Parameter(torch.randn(max_len, hidden_dim))\n        self.img_proj = nn.Linear(feature_dim, hidden_dim)\n\n        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead, activation='gelu', batch_first=True)\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, image_tokens, caption_tokens):\n        B, T = caption_tokens.shape\n        tgt = self.token_embed(caption_tokens) + self.pos_embed[:T]\n        memory = self.img_proj(image_tokens)\n\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(caption_tokens.device)\n        return self.fc_out(self.decoder(tgt, memory, tgt_mask=tgt_mask))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "83527fbc",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": "# CELL 5 - ENCODER + DECODER \n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, images, captions):\n        image_tokens = self.encoder(images)           # (B, 64, C)\n        return self.decoder(image_tokens, captions)   # (B, T, vocab_size)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e0942f1a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/tmp/ipykernel_3515/4127236421.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n"
        }
      ],
      "source": "# CELL 6 - LOSS, OPTIMIZER & AMP \n\nPAD_ID = word2idx[\"<PAD>\"]\n\ndecoder = CaptionDecoder(\n    vocab_size=len(word2idx),\n    feature_dim=encoder(torch.randn(1, 3, 256, 256).to(device)).shape[-1]\n).to(device)\n\nmodel = ImageCaptioningModel(encoder, decoder).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n\noptimizer = torch.optim.AdamW([\n    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n    {\"params\": model.decoder.parameters(), \"lr\": 1e-4}\n])\n\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0cb59acd",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/tmp/ipykernel_3515/2849251408.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_3515/2849251408.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\ud83e\udde0 Step 000 | Loss: 9.7175 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 005 | Loss: 8.1538 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 010 | Loss: 7.3898 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 015 | Loss: 7.0409 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 020 | Loss: 6.7757 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 025 | Loss: 6.5165 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 030 | Loss: 6.2754 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 035 | Loss: 6.0958 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 040 | Loss: 5.9553 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 045 | Loss: 5.8152 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 050 | Loss: 5.6288 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 055 | Loss: 5.4940 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 060 | Loss: 5.4055 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 065 | Loss: 5.3146 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 070 | Loss: 5.2387 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 075 | Loss: 5.1555 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 080 | Loss: 5.0369 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 085 | Loss: 5.0171 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 090 | Loss: 4.8977 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 095 | Loss: 4.8328 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 100 | Loss: 4.7934 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 105 | Loss: 4.7507 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 110 | Loss: 4.7290 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 115 | Loss: 4.6598 | GPU Mem: 1.62 GB\n\n\u2705 Epoch 1/10 | Avg Loss: 5.809684\n\n\ud83d\udcbe Checkpoint saved to checkpoints/captioning_latest.pth\n\ud83e\udde0 Step 000 | Loss: 4.6206 | GPU Mem: 2.22 GB\n\ud83e\udde0 Step 005 | Loss: 4.5940 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 010 | Loss: 4.5894 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 015 | Loss: 4.5344 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 020 | Loss: 4.4488 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 025 | Loss: 4.3859 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 030 | Loss: 4.3951 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 035 | Loss: 4.3337 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 040 | Loss: 4.2768 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 045 | Loss: 4.2232 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 050 | Loss: 4.2042 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 055 | Loss: 4.1765 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 060 | Loss: 4.1255 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 065 | Loss: 4.0819 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 070 | Loss: 4.0907 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 075 | Loss: 4.0267 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 080 | Loss: 3.9702 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 085 | Loss: 3.9764 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 090 | Loss: 3.9857 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 095 | Loss: 3.8756 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 100 | Loss: 3.9496 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 105 | Loss: 3.8111 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 110 | Loss: 3.7901 | GPU Mem: 2.23 GB\n\ud83e\udde0 Step 115 | Loss: 3.8075 | GPU Mem: 1.62 GB\n\n\u2705 Epoch 2/10 | Avg Loss: 4.171448\n\n\ud83d\udcbe Checkpoint saved to checkpoints/captioning_latest.pth\n\ud83e\udde0 Step 000 | Loss: 3.8314 | GPU Mem: 2.22 GB\n"
        }
      ],
      "source": "# CELL 7 - TRAINING LOOP\n\nimport os\nfrom torch.cuda.amp import autocast, GradScaler\n\nEPOCHS = 10\nCHECKPOINT_DIR = \"checkpoints\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\ncheckpoint_path = os.path.join(CHECKPOINT_DIR, \"captioning_latest.pth\")\n\n# \ud83d\udd01 Resume logic\nstart_epoch = 0\nscaler = GradScaler()\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n    start_epoch = checkpoint[\"epoch\"]\n    print(f\"\ud83d\udd04 Resuming training from epoch {start_epoch}\")\n\n# \ud83c\udfcb\ufe0f Training loop\nfor epoch in range(start_epoch, EPOCHS):\n    model.train()\n    total_loss = 0.0\n\n    for step, (images, captions) in enumerate(loader):\n        images, captions = images.to(device), captions.to(device)\n        optimizer.zero_grad()\n\n        with autocast():\n            outputs = model(images, captions[:, :-1])  # input\n            loss = criterion(\n                outputs.reshape(-1, outputs.size(-1)),\n                captions[:, 1:].reshape(-1)\n            )\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n        # \u2705 Live GPU memory print (once per 5 steps)\n        if step % 5 == 0:\n            mem_gb = torch.cuda.memory_allocated() / 1024**3\n            print(f\"\ud83e\udde0 Step {step:03d} | Loss: {loss.item():.4f} | GPU Mem: {mem_gb:.2f} GB\")\n\n    avg_loss = total_loss / len(loader)\n    print(f\"\\n\u2705 Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.6f}\\n\")\n\n    # \ud83d\udcbe Save checkpoint after each epoch\n    ckpt = {\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'config': {\n            'vocab_size': len(word2idx),\n            'hidden_dim': 512,\n            'num_heads': 8,\n            'num_layers': 6\n        }\n    }\n    torch.save(ckpt, checkpoint_path)\n    print(f\"\ud83d\udcbe Checkpoint saved to {checkpoint_path}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7a4d14c2-7b78-44cd-8b74-f2d3eec90567",
      "metadata": {},
      "outputs": [],
      "source": "import torch\nfrom model import Encoder  # use your actual Encoder class definition\n\n# Load full training checkpoint\nckpt = torch.load(\"checkpoints/captioning_latest.pth\", map_location=\"cpu\")\n\n# Rebuild encoder\nencoder = Encoder()\nencoder.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n\n# Save entire encoder object as .pt (not just weights)\ntorch.save(encoder, \"encoder_epoch_52.pt\")\nprint(\"\u2705 Saved fine-tuned encoder to encoder_epoch_52.pt\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81eacc46-3d22-40b9-8c30-2cb0fc10de26",
      "metadata": {},
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
