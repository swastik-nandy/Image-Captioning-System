{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b71ade",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "üî• Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL 0 - SELECT DEVICE & LOAD DEPENDENCIES\n",
    "\n",
    "!pip install -q timm\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import timm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üî• Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474934f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Patch token shape: torch.Size([1, 64, 256])\n",
      "‚úÖ Encoder weights loaded with strict=True\n",
      "üß† Encoder ready (feature dim = 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3515/3045962482.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=True)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Encoder Only (decoder-ready, no projection head)\n",
    "\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ‚úÖ Encoder that returns patch-wise feature map: (B, 64, C)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"efficientnetv2_s\",\n",
    "            pretrained=False,\n",
    "            features_only=True\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # ensures 8√ó8 spatial output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)[-1]              # (B, C, H, W) ‚Üí final block\n",
    "        x = self.pool(x)                      # (B, C, 8, 8)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1)             # (B, 8, 8, C)\n",
    "        x = x.reshape(B, H * W, C)            # (B, 64, C)\n",
    "        return x\n",
    "\n",
    "# Instantiate encoder first\n",
    "encoder = Encoder().to(device)\n",
    "\n",
    "# üîç Dynamically detect feature dim (C) from dummy input\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, 256, 256).to(device)\n",
    "    out = encoder(dummy)\n",
    "    feature_dim = out.shape[-1]  # usually 1280 for effnetv2_s\n",
    "    print(f\"üß™ Patch token shape: {out.shape}\")  # (1, 64, 1280)\n",
    "\n",
    "# üíæ Load encoder weights with fallback\n",
    "try:\n",
    "    encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=True)\n",
    "    print(\"‚úÖ Encoder weights loaded with strict=True\")\n",
    "except RuntimeError as e:\n",
    "    print(\"‚ö†Ô∏è Strict loading failed:\", e)\n",
    "    print(\"üîÅ Retrying with strict=False\")\n",
    "    encoder.load_state_dict(torch.load(\"encoder_epoch_50.pt\", map_location=device), strict=False)\n",
    "\n",
    "print(f\"üß† Encoder ready (feature dim = {feature_dim})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bea99b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 2 - CAPTION PROCESSING\n",
    "\n",
    "\n",
    "with open(\"captions_train2017.json\", 'r') as f:\n",
    "    annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "captions_dict = {}\n",
    "for ann in annotations:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    cap = ann[\"caption\"]\n",
    "    captions_dict.setdefault(img_id, []).append(cap)\n",
    "\n",
    "# Basic tokenizer & vocab\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "word_freq = {}\n",
    "for caps in captions_dict.values():\n",
    "    for cap in caps:\n",
    "        for token in tokenize(cap):\n",
    "            word_freq[token] = word_freq.get(token, 0) + 1\n",
    "\n",
    "# Build vocab\n",
    "vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "for word, freq in word_freq.items():\n",
    "    if freq >= 5:  # min frequency cutoff\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "word2idx = vocab\n",
    "idx2word = {idx: word for word, idx in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"word2idx\": word2idx, \"idx2word\": idx2word}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9249f382",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 3 - DATASET + DATALOADER\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # ‚úÖ allows loading broken JPEGs\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, folder, image2caption, word2idx, transform):\n",
    "        self.folder = folder\n",
    "        self.mapping = list(image2caption.items())\n",
    "        self.word2idx = word2idx\n",
    "        self.transform = transform\n",
    "        self.fallback_count = 0  # üß† optional: track black image fallbacks\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_id, captions = self.mapping[i]\n",
    "        img_path = os.path.join(self.folder, f\"{img_id:012}.jpg\")\n",
    "\n",
    "        # ‚úÖ Safe image load with grayscale/corrupt fallback\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            self.fallback_count += 1\n",
    "            print(f\"‚ö†Ô∏è Fallback: Could not load {img_path} ‚Äî {e}\")\n",
    "            image = Image.new(\"RGB\", (256, 256), color=(0, 0, 0))  # black dummy\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # üßæ Caption tokenization\n",
    "        caption = [\"<SOS>\"] + random.choice(captions).lower().strip().split() + [\"<EOS>\"]\n",
    "        tokens = [self.word2idx.get(w.strip(\".,!?\"), self.word2idx[\"<UNK>\"]) for w in caption]\n",
    "        tokens = tokens[:20] + [self.word2idx[\"<PAD>\"]] * (20 - len(tokens))\n",
    "\n",
    "        return image, torch.tensor(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "# ‚úÖ Standard transform (resize, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# ‚úÖ Instantiate dataset\n",
    "dataset = ImageCaptionDataset(\"images\", captions_dict, word2idx, transform)\n",
    "\n",
    "# ‚úÖ FINAL DataLoader with high batch size + PIL-safe settings\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,      # üß† set to 1024 if you tested and confirmed it's safe\n",
    "    shuffle=True,\n",
    "    num_workers=0,       # ‚úÖ Single-threaded (PIL-safe)\n",
    "    pin_memory=False     # ‚úÖ Avoid async GPU transfer issues\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32dfb3bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CELL 4 - TRANSFORMER DECODER\n",
    "\n",
    "class CaptionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim, hidden_dim=512, num_layers=6, nhead=8, max_len=20):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(max_len, hidden_dim))\n",
    "        self.img_proj = nn.Linear(feature_dim, hidden_dim)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead, activation='gelu', batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, image_tokens, caption_tokens):\n",
    "        B, T = caption_tokens.shape\n",
    "        tgt = self.token_embed(caption_tokens) + self.pos_embed[:T]\n",
    "        memory = self.img_proj(image_tokens)\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(caption_tokens.device)\n",
    "        return self.fc_out(self.decoder(tgt, memory, tgt_mask=tgt_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83527fbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 5 - ENCODER + DECODER \n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        image_tokens = self.encoder(images)           # (B, 64, C)\n",
    "        return self.decoder(image_tokens, captions)   # (B, T, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0942f1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3515/4127236421.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 - LOSS, OPTIMIZER & AMP \n",
    "\n",
    "PAD_ID = word2idx[\"<PAD>\"]\n",
    "\n",
    "decoder = CaptionDecoder(\n",
    "    vocab_size=len(word2idx),\n",
    "    feature_dim=encoder(torch.randn(1, 3, 256, 256).to(device)).shape[-1]\n",
    ").to(device)\n",
    "\n",
    "model = ImageCaptioningModel(encoder, decoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.decoder.parameters(), \"lr\": 1e-4}\n",
    "])\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb59acd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3515/2849251408.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_3515/2849251408.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Step 000 | Loss: 9.7175 | GPU Mem: 2.23 GB\n",
      "üß† Step 005 | Loss: 8.1538 | GPU Mem: 2.23 GB\n",
      "üß† Step 010 | Loss: 7.3898 | GPU Mem: 2.23 GB\n",
      "üß† Step 015 | Loss: 7.0409 | GPU Mem: 2.23 GB\n",
      "üß† Step 020 | Loss: 6.7757 | GPU Mem: 2.23 GB\n",
      "üß† Step 025 | Loss: 6.5165 | GPU Mem: 2.23 GB\n",
      "üß† Step 030 | Loss: 6.2754 | GPU Mem: 2.23 GB\n",
      "üß† Step 035 | Loss: 6.0958 | GPU Mem: 2.23 GB\n",
      "üß† Step 040 | Loss: 5.9553 | GPU Mem: 2.23 GB\n",
      "üß† Step 045 | Loss: 5.8152 | GPU Mem: 2.23 GB\n",
      "üß† Step 050 | Loss: 5.6288 | GPU Mem: 2.23 GB\n",
      "üß† Step 055 | Loss: 5.4940 | GPU Mem: 2.23 GB\n",
      "üß† Step 060 | Loss: 5.4055 | GPU Mem: 2.23 GB\n",
      "üß† Step 065 | Loss: 5.3146 | GPU Mem: 2.23 GB\n",
      "üß† Step 070 | Loss: 5.2387 | GPU Mem: 2.23 GB\n",
      "üß† Step 075 | Loss: 5.1555 | GPU Mem: 2.23 GB\n",
      "üß† Step 080 | Loss: 5.0369 | GPU Mem: 2.23 GB\n",
      "üß† Step 085 | Loss: 5.0171 | GPU Mem: 2.23 GB\n",
      "üß† Step 090 | Loss: 4.8977 | GPU Mem: 2.23 GB\n",
      "üß† Step 095 | Loss: 4.8328 | GPU Mem: 2.23 GB\n",
      "üß† Step 100 | Loss: 4.7934 | GPU Mem: 2.23 GB\n",
      "üß† Step 105 | Loss: 4.7507 | GPU Mem: 2.23 GB\n",
      "üß† Step 110 | Loss: 4.7290 | GPU Mem: 2.23 GB\n",
      "üß† Step 115 | Loss: 4.6598 | GPU Mem: 1.62 GB\n",
      "\n",
      "‚úÖ Epoch 1/10 | Avg Loss: 5.809684\n",
      "\n",
      "üíæ Checkpoint saved to checkpoints/captioning_latest.pth\n",
      "üß† Step 000 | Loss: 4.6206 | GPU Mem: 2.22 GB\n",
      "üß† Step 005 | Loss: 4.5940 | GPU Mem: 2.23 GB\n",
      "üß† Step 010 | Loss: 4.5894 | GPU Mem: 2.23 GB\n",
      "üß† Step 015 | Loss: 4.5344 | GPU Mem: 2.23 GB\n",
      "üß† Step 020 | Loss: 4.4488 | GPU Mem: 2.23 GB\n",
      "üß† Step 025 | Loss: 4.3859 | GPU Mem: 2.23 GB\n",
      "üß† Step 030 | Loss: 4.3951 | GPU Mem: 2.23 GB\n",
      "üß† Step 035 | Loss: 4.3337 | GPU Mem: 2.23 GB\n",
      "üß† Step 040 | Loss: 4.2768 | GPU Mem: 2.23 GB\n",
      "üß† Step 045 | Loss: 4.2232 | GPU Mem: 2.23 GB\n",
      "üß† Step 050 | Loss: 4.2042 | GPU Mem: 2.23 GB\n",
      "üß† Step 055 | Loss: 4.1765 | GPU Mem: 2.23 GB\n",
      "üß† Step 060 | Loss: 4.1255 | GPU Mem: 2.23 GB\n",
      "üß† Step 065 | Loss: 4.0819 | GPU Mem: 2.23 GB\n",
      "üß† Step 070 | Loss: 4.0907 | GPU Mem: 2.23 GB\n",
      "üß† Step 075 | Loss: 4.0267 | GPU Mem: 2.23 GB\n",
      "üß† Step 080 | Loss: 3.9702 | GPU Mem: 2.23 GB\n",
      "üß† Step 085 | Loss: 3.9764 | GPU Mem: 2.23 GB\n",
      "üß† Step 090 | Loss: 3.9857 | GPU Mem: 2.23 GB\n",
      "üß† Step 095 | Loss: 3.8756 | GPU Mem: 2.23 GB\n",
      "üß† Step 100 | Loss: 3.9496 | GPU Mem: 2.23 GB\n",
      "üß† Step 105 | Loss: 3.8111 | GPU Mem: 2.23 GB\n",
      "üß† Step 110 | Loss: 3.7901 | GPU Mem: 2.23 GB\n",
      "üß† Step 115 | Loss: 3.8075 | GPU Mem: 1.62 GB\n",
      "\n",
      "‚úÖ Epoch 2/10 | Avg Loss: 4.171448\n",
      "\n",
      "üíæ Checkpoint saved to checkpoints/captioning_latest.pth\n",
      "üß† Step 000 | Loss: 3.8314 | GPU Mem: 2.22 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     27\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mImageCaptionDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è Fallback: Could not load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ‚Äî \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# black dummy\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# üßæ Caption tokenization\u001b[39;00m\n\u001b[1;32m     29\u001b[0m caption \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<SOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(captions)\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<EOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CELL 7 - TRAINING LOOP\n",
    "\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "EPOCHS = 10\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"captioning_latest.pth\")\n",
    "\n",
    "# üîÅ Resume logic\n",
    "start_epoch = 0\n",
    "scaler = GradScaler()\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    print(f\"üîÑ Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "# üèãÔ∏è Training loop\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, (images, captions) in enumerate(loader):\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(images, captions[:, :-1])  # input\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.size(-1)),\n",
    "                captions[:, 1:].reshape(-1)\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ‚úÖ Live GPU memory print (once per 5 steps)\n",
    "        if step % 5 == 0:\n",
    "            mem_gb = torch.cuda.memory_allocated() / 1024**3\n",
    "            print(f\"üß† Step {step:03d} | Loss: {loss.item():.4f} | GPU Mem: {mem_gb:.2f} GB\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"\\n‚úÖ Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.6f}\\n\")\n",
    "\n",
    "    # üíæ Save checkpoint after each epoch\n",
    "    ckpt = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'config': {\n",
    "            'vocab_size': len(word2idx),\n",
    "            'hidden_dim': 512,\n",
    "            'num_heads': 8,\n",
    "            'num_layers': 6\n",
    "        }\n",
    "    }\n",
    "    torch.save(ckpt, checkpoint_path)\n",
    "    print(f\"üíæ Checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4d14c2-7b78-44cd-8b74-f2d3eec90567",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoder  \u001b[38;5;66;03m# use your actual Encoder class definition\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load full training checkpoint\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/captioning_latest.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import Encoder  # use your actual Encoder class definition\n",
    "\n",
    "# Load full training checkpoint\n",
    "ckpt = torch.load(\"checkpoints/captioning_latest.pth\", map_location=\"cpu\")\n",
    "\n",
    "# Rebuild encoder\n",
    "encoder = Encoder()\n",
    "encoder.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "\n",
    "# Save entire encoder object as .pt (not just weights)\n",
    "torch.save(encoder, \"encoder_epoch_52.pt\")\n",
    "print(\"‚úÖ Saved fine-tuned encoder to encoder_epoch_52.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eacc46-3d22-40b9-8c30-2cb0fc10de26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
